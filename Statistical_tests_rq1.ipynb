{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245903ee",
   "metadata": {},
   "source": [
    "## Pairwise McNemars test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Load your predictions and ground truth\n",
    "y_true = np.load(\"y_test.npy\")\n",
    "y_pred_lr = np.load(\"y_pred_lr.npy\")\n",
    "y_pred_rf = np.load(\"y_pred_rf.npy\")\n",
    "y_pred_svm = np.load(\"y_pred_svm.npy\")\n",
    "\n",
    "# Create a baseline model that always predicts class 1\n",
    "y_pred_baseline = np.ones_like(y_true)\n",
    "\n",
    "# Define the McNemar test function\n",
    "def run_mcnemar(y_true, pred_a, pred_b, name_a=\"Model A\", name_b=\"Model B\"):\n",
    "    # Contingency table: [[both correct, a correct only], [b correct only, both wrong]]\n",
    "    a_correct = pred_a == y_true\n",
    "    b_correct = pred_b == y_true\n",
    "\n",
    "    both_correct = np.sum(a_correct & b_correct)\n",
    "    a_only = np.sum(a_correct & ~b_correct)\n",
    "    b_only = np.sum(~a_correct & b_correct)\n",
    "    both_wrong = np.sum(~a_correct & ~b_correct)\n",
    "\n",
    "    table = [[both_correct, a_only],\n",
    "             [b_only, both_wrong]]\n",
    "\n",
    "    print(f\"\\nMcNemar's Test: {name_a} vs {name_b}\")\n",
    "    print(\"Contingency Table:\", table)\n",
    "\n",
    "    result = mcnemar(table, exact=False, correction=True)\n",
    "    print(f\"Statistic = {result.statistic:.4f}, p-value = {result.pvalue:.40f}\")\n",
    "\n",
    "# Run comparisons\n",
    "run_mcnemar(y_true, y_pred_lr, y_pred_rf, \"Logistic Regression\", \"Random Forest\")\n",
    "run_mcnemar(y_true, y_pred_lr, y_pred_svm, \"Logistic Regression\", \"SVM\")\n",
    "run_mcnemar(y_true, y_pred_rf, y_pred_svm, \"Random Forest\", \"SVM\")\n",
    "\n",
    "# Compare each model to baseline\n",
    "run_mcnemar(y_true, y_pred_lr, y_pred_baseline, \"Logistic Regression\", \"Baseline\")\n",
    "run_mcnemar(y_true, y_pred_rf, y_pred_baseline, \"Random Forest\", \"Baseline\")\n",
    "run_mcnemar(y_true, y_pred_svm, y_pred_baseline, \"SVM\", \"Baseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf24ab6",
   "metadata": {},
   "source": [
    "# Bootstrapping for precision intervals  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f62aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load predictions and ground truth\n",
    "y_true = np.load(\"y_test.npy\")\n",
    "y_pred_lr = np.load(\"y_pred_lr.npy\")\n",
    "y_pred_rf = np.load(\"y_pred_rf.npy\")\n",
    "y_pred_svm = np.load(\"y_pred_svm.npy\")\n",
    "\n",
    "# Baseline: predict all class 1\n",
    "y_pred_baseline = np.ones_like(y_true)\n",
    "\n",
    "# Bootstrapping function\n",
    "def bootstrap_precision(y_true, y_pred, n_bootstraps=1000, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    precision_scores = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
    "        y_true_sample = y_true[indices]\n",
    "        y_pred_sample = y_pred[indices]\n",
    "        try:\n",
    "            score = precision_score(y_true_sample, y_pred_sample, pos_label=1)\n",
    "        except:\n",
    "            score = 0.0  # In case no positive predictions exist\n",
    "        precision_scores.append(score)\n",
    "\n",
    "    return np.mean(precision_scores), np.percentile(precision_scores, [2.5, 97.5])\n",
    "\n",
    "# Run for each model\n",
    "models = {\n",
    "    \"Logistic Regression\": y_pred_lr,\n",
    "    \"Support Vector Machine\": y_pred_svm,\n",
    "    \"Random Forest\": y_pred_rf,\n",
    "    \"Baseline\": y_pred_baseline,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, preds in models.items():\n",
    "    mean_prec, ci = bootstrap_precision(y_true, preds)\n",
    "    results[model_name] = (mean_prec, ci)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"Model\\t\\t\\tMean Precision\\t\\t95% CI\")\n",
    "for model, (mean_prec, ci) in results.items():\n",
    "    print(f\"{model:25s} {mean_prec:.3f} \\t\\t [{ci[0]:.3f}, {ci[1]:.3f}]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
